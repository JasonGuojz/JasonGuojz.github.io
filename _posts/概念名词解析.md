1. Dirichlet 过程   [参考1 掘金](https://juejin.im/entry/58e09c2cda2f60005fcd5573) 

2. 贝叶斯推断

3. 贝叶斯网络  [Uncertainty in Profit Scoring (Bayesian Deep Learning)](https://humboldt-wi.github.io/blog/research/information_systems_1819/uncertainty-and-credit-scoring/)  [知乎大佬Bayesian Neural Networks：贝叶斯神经网络](https://zhuanlan.zhihu.com/p/81170602)

4. 无偏蒙特卡洛梯度 [参考]([https://www.google.com/search?q=%E6%97%A0%E5%81%8F%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A2%AF%E5%BA%A6&oq=%E6%97%A0%E5%81%8F%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A2%AF%E5%BA%A6&aqs=chrome..69i57.408j0j1&sourceid=chrome&ie=UTF-8](https://www.google.com/search?q=无偏蒙特卡洛梯度&oq=无偏蒙特卡洛梯度&aqs=chrome..69i57.408j0j1&sourceid=chrome&ie=UTF-8))

5. 蒙特卡罗方法（Monte Carlo method）[Reinforcement-Learning-Monte-Carlo](https://oneraynyday.github.io/ml/2018/05/24/Reinforcement-Learning-Monte-Carlo/)

6. 蒙特卡洛dropout[What is Monte Carlo dropout?](https://datascience.stackexchange.com/questions/44065/what-is-monte-carlo-dropout)

7. 蒙特卡洛积分 [Monte Carlo数学原理](https://zhuanlan.zhihu.com/p/61611088)  [随机模拟-Monte Carlo积分及采样（详述直接采样、接受-拒绝采样、重要性采样）](https://www.jianshu.com/p/3d30070932a8)

8. Dropout variational inference Dropout变分推理

9. [reference—深度学习中的两种不确定性](https://zhuanlan.zhihu.com/p/56986840)

10. [*What My Deep Model Doesn't Know*](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html)

   * prediction uncertainty 预测不确定性

11. 变分贝叶斯方法   [cnblog讲解](https://blog.csdn.net/aws3217150/article/details/57072827)

12. 深度学习 回归任务 高斯噪声

13. prediction uncertainty 在分类问题中，预测不确定性可以利用蒙特卡洛积分来近似

14. 混合尺度高斯先验（scale mixture gaussian prior）

15. 自动编码器

    自动编码器的基本问题在于，它们将其输入转换成其编码矢量，其所在的潜在空间可能不连续，或者允许简单的插值。

16. 变分自动编码器

17. 变分推理variational inference [变分推理]([http://blog.rexking6.top/2019/05/30/%E5%8F%98%E5%88%86%E6%8E%A8%E7%90%86/](http://blog.rexking6.top/2019/05/30/变分推理/))   [变分推断 贝叶斯神经网络有什么论文可以推荐阅读吗？](https://www.zhihu.com/question/324099754/answer/696409097)

18. 残差

    * 普通残差：

      <img src="%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200806103226995.png" alt="image-20200806103226995"  />

    * 残差图：估计观察或预测到的误差error(残差residuals)与随机误差(stochastic error)是否一致

19. **减弱 错误标签 的影响** [PaperReading：Learning with Noisy Label-深度学习廉价落地](https://zhuanlan.zhihu.com/p/110959020)

20. [利用不确定性来衡量多任务学习中的损失函数](https://zhuanlan.zhihu.com/p/65137250)

21. 高斯过程 [CS229——Gaussian processes](https://kivy-cn.github.io/Stanford-CS-229-CN/#/Markdown/cs229-gaussian_processes)       [cornell——Lecture 15: Gaussian Processes](cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote15.html)     [高斯过程Gaussian Process教程](https://www.kesci.com/home/project/5d8da105037db3002d3a4c4a)     [krasserm blog——code contained](http://krasserm.github.io/2018/03/19/gaussian-processes/)

    [Gaussian Processes for Machine Learning——book](http://www.gaussianprocess.org/gpml/chapters/)

22. 深度高斯过程

23. 点估计 区间估计 

24. [贝叶斯推断之最大后验概率(MAP)](https://www.cnblogs.com/hapjin/p/8834794.html)     花书 19.3节

25. Epsilon greedy search 

26. Confidence calibration 置信度校正    模型的校正度：

    * 校正的目的是 makes the confidence scores reflect true probabilities.
    * A simple way to visualize calibration is plotting accuracy as a function of confidence (known as a **[reliability diagram](http://www.datascienceassn.org/sites/default/files/Predicting good probabilities with supervised learning.pdf)**). 
    * “On Calibration of Modern Neural Networks”

27. [What is the meaning of the word logits in TensorFlow?](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow)      logits    [What does Logits in machine learning mean?](https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean)

28. Batch Normalization [第十节——Batch Normalization]([http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapters/4_optimization.html](http://www.huaxiaozhuan.com/深度学习/chapters/4_optimization.html))

29. Isotonic Regression   [使用 Isotonic Regression 校准分类器]([http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/21/classifier-calibration-with-isotonic-regression](http://vividfree.github.io/机器学习/2015/12/21/classifier-calibration-with-isotonic-regression))

30. 共轭梯度法  [共轭梯度法的简单分析]([https://alkane0050.fun/2019/05/18/%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%E5%88%9D%E6%AD%A5/](https://alkane0050.fun/2019/05/18/共轭梯度法初步/))

31. 远程监督：主要是对知识库与非结构化文本对齐来自动构建大量训练数据，减少模型对人工标注数据的依赖，增强模型跨领域适应能力。但有  noise  problem 

    **基本假设：**两个实体如果在知识库中存在某种关系，则包含该两个实体的非结构化句子均能表示出这种关系

    [知识抽取-实体及关系抽取—DS](https://zhuanlan.zhihu.com/p/44772023)

32. Knowledge Graph

    * Entity-Centric Knowledge Graph

      ![image-20200808170218438](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200808170218438.png)

      <img src="%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200808170106351.png" alt="image-20200808170106351" style="zoom:80%;" />

    * Event-Centric Knowledge Graph

      ![image-20200808170316162](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200808170316162.png)

    * 

33. 事件提取  Event Extraction

    定义： Identify the relation between $\color{red}{an\ event\ and\ an\ entity}$

    Event定义：An event is defined as a specific occurrence involving participants

    要找到Event trigger, Event Type, Event argument, Argument role

    Event 一般与 trigger有紧密关系(Event Identification(TriggerWords))，且 trigger一般为 verb 

    ![image-20200808170633114](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200808170633114.png)

    ![image-20200808170527436](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200808170527436.png)

34. 开放域事件提取 Open Domain Event Extraction

    * **Features  Representation** 

      * Traditional Methods for Feature Representation

        *  Human designed features
        * Too much rely on imprecise NLP tools for feature extraction
        * Limitations for low-resources languages

      * Dynamic CNN

      * Argument Attention(Event arguments) 

        arguments 识别对Event Detection有很大帮助

        If we consider the argument phrase “former protege” (Role=Position), we will have more confidence to predict it as an End-·ition event

        * 从 contextual words 和 entities 的信息找 arguments—— context representation learning(CRL) 学到 contextual words 和 entities的representation(embedding或是其它)，与对应的attention $\alpha$ 内积

    * Training Data Generation

      * External Resources
      
        * Employing **FrameNet**( semantic role descriptions in FrameNet, VerbNet (Kipper etal., 2008) and Propbank (Palmer et al., 2005).) [FrameNet & FrameNet Python API ](https://blog.csdn.net/qq_36771895/article/details/91355166)
      
          <img src="%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200813154936884.png" alt="image-20200813154936884" style="zoom:80%;" />
      
          ![image-20200809095354724](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200809095354724.png)
      
          ![image-20200809110609620](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200809110609620.png)
      
          **How to generate training data in FrameNet**
      
          - 方法1 [Open Domain Event Extraction from Texts]![image-20200809101143166](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200809101143166.png)
      
            对生成数据准确度的评价
      
            ![image-20200809101547986](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200809101547986.png)
      
          - 
      
        * Employing **Freebase** 2016年，谷歌宣布将Freebase的数据和API服务都迁移至Wikidata，并正式关闭了Freebase  [知识图谱调研-Freebase](https://developer.aliyun.com/article/717320)
      
      * Generating Labeled Data from **Structured KB**
      
        * Distant Supervision(Weak) Supervision in **Relation Extraction**($\color{red}{doesn’t}$ work for Event Extraction)
      
          ![image-20200809101755184](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200809101755184.png)
      
          `Automatically Labeled Data Generation for Large Scale Event Extraction`文中对Freebase 的介绍
      
          <img src="%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200813153441759.png" alt="image-20200813153441759" style="zoom:80%;" />
      
        * **Triggers** are not given out in existing knowledge bases 所以没法直接用existing  **Structured KB**
      
          ![image-20200809103434298](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200809103434298.png)
      
          所以可以根据**Structured KB**中的 Key Arguments label back，依据假设提取Trigger：
      
          1. Event **Trigger Words Extraction**
      
             假设：The sentences mention **all arguments denote such events**
      
          2. Argument Extraction/Role Identification
      
             根据 Trigger words and Entities
      
             语言学上的规律：Arguments for a specific event instance are usually mentioned in multiple sentences，**Only 0.02% of instances can find all argument mentions in one sentence**
        
        * method
        
          1. ![image-20200809110639296](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200809110639296.png)

35. 关系抽取 Relation Extraction

    定义： Identify the relation between $\color{}{}$ $\color{red}{two\ given\ entities}$

    ![image-20200808171310420](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200808171310420.png)

    [实体及关系抽取](https://zhuanlan.zhihu.com/p/44772023)

    [事件抽取](https://zhuanlan.zhihu.com/p/50903358)

36. Event Extraction(EE) 事件提取

    [reference1—chriszhangcx blog]([https://chriszhangcx.github.io/%E5%A6%82%E4%BD%95%E7%8C%9C%E5%87%BA%E5%A5%B3%E6%9C%8B%E5%8F%8B%E8%AF%9D%E4%B8%AD%E7%9A%84%E5%90%AB%E4%B9%89%EF%BC%9F%E2%80%94%E2%80%94%E5%85%B3%E4%BA%8E%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96-Event-Extraction/#fn_%E5%8D%9A%E5%AE%A2](https://chriszhangcx.github.io/如何猜出女朋友话中的含义？——关于事件抽取-Event-Extraction/#fn_博客)) and [2—Introduction of Event Extraction](http://www.caojiarun.com/2019/11/Introduction-of-Event-Extraction/)

    [A Survey of Open Domain Event Extraction](https://pdfs.semanticscholar.org/0eef/643c744ac3e4ffd68d4328b5f445dbf9e10e.pdf)

    [事件抽取(Event Extraction)经典模型](https://chriszhangcx.github.io/如何猜出女朋友话中的含义？——关于事件抽取-Event-Extraction/)

37. POS tagged pos标记[Part of Speech (PoS) Tagging](https://www.tutorialspoint.com/natural_language_processing/natural_language_processing_part_of_speech_tagging.htm)

    * 概率方法 (CRF GMM) [Identifying Part of Speech Tags using Conditional Random Fields](https://medium.com/analytics-vidhya/pos-tagging-using-conditional-random-fields-92077e5eaa31)

38. ACE Corpus：ACE2005: 529 Training, 33 Development, 40 Testing

39. NLTK [Natural Language Toolkit3.5](https://www.nltk.org/)

40. [Attention Mechanism](https://blog.floydhub.com/attention-mechanism

41. 截断梯度法  [TRUNCATED GRADIENT ](https://zr9558.com/2016/01/12/truncated-gradient/)

42. burn-in (Gibbs sampling) [Burn-In is Unnecessary](http://users.stat.umn.edu/~geyer/mcmc/burn.html)

43. LDA 主题模型 [一文详解LDA主题模型](https://zhuanlan.zhihu.com/p/31470216)

44.  [G-test](https://en.wikipedia.org/wiki/G-test)

45. 共指和指代消解 [Coreference Resolution]([https://looperxx.github.io/CS224n-2019-16-Coreference%20Resolution/](https://looperxx.github.io/CS224n-2019-16-Coreference Resolution/))

46. semantic role labeling representation(SRL) [Semantic    Role    Labeling](https://web.stanford.edu/~jurafsky/slp3/20.pdf)

     meaning  representations：Abstract  Meaning  Representation  (AMR)、Stanford  Typed  Dependencies 、FrameNet  [Meaning Representation and SRL: assuming there is some meaning](https://towardsdatascience.com/meaning-representation-and-srl-assuming-there-is-some-meaning-741f35bfdd6)

    **[Advanced Semantic Representation](http://people.cs.georgetown.edu/nschneid/cosc672/s17/amr-papers.html)**

    **[AMR Tutorial](https://github.com/nschneid/amr-tutorial/)**

    [Abstract Meaning Representation (AMR) 1.2Specification](https://github.com/amrisi/amr-guidelines/blob/master/amr.md)

47. [综述 | 事件抽取及推理 (上)](http://blog.openkg.cn/%E7%BB%BC%E8%BF%B0-%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E5%8F%8A%E6%8E%A8%E7%90%86-%E4%B8%8A/)

48. word sense 词的意思[Word sense](https://en.wikipedia.org/wiki/Word_sense)

49. [NLP的任务](https://zhuanlan.zhihu.com/p/109122090)

50. Word2Vec — [Skip-Gram]([http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapters/8_word_representation.html](http://www.huaxiaozhuan.com/深度学习/chapters/8_word_representation.html)) and CBOW 

51. WordNet sense 到 [OntoNotes sense](https://catalog.ldc.upenn.edu/LDC2013T19)的 mapping tool 

52. [Ontonotes Sense Groups](http://clear.colorado.edu/compsem/index.php?page=lexicalresources&sub=ontonotes)

53. 分布语义，Distributional Semantic Representation，基于分布假设：*linguistic items with similar distributions have similar meanings.*

54. GRU   [动手深度学习GRU](https://zh.d2l.ai/chapter_recurrent-neural-networks/gru.html)

55. DAG有向无环图：

56. syntactic parsing语法分析:

    * 短语结构树(phrase structure tree 对应语法 context-free grammar CFG 上下文无关法)
    * 依存句法树(dependency parse tree)： 直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各语法成分之间的关系。

57. 语义依存分析 (Semantic Dependency Parsing, SDP)：分析句子各个语言单位之间的语义关联，并将语义关联以依存结构呈现

    [依存句法分析与语义依存分析的区别](https://my.oschina.net/duanvincent/blog/761108)

    [Stanford-parser依存句法关系解释](https://blog.csdn.net/glory1234work2115/article/details/54906343)

58. Distributional Representation和Distributed Representation [聊聊文本的分布式表示—邱锡鹏](https://zhuanlan.zhihu.com/p/22386230)

59. **ACE2005:** 

    ACE2005定义的事件抽取是：(1) 以句子级为单位，识别句子中出现的trigger词及类型，(2) 针对每个trigger词，判断其的论元argument以及论元类型。下图即是ACE2005任务的一个示例。

60. RNN及变体和BPTT [RNN 其常见架构](https://zhuanlan.zhihu.com/p/27485750)

61. dependency parsing [笔记1](https://looperxx.github.io/CS224n-2019-05-Linguistic Structure Dependency Parsing/)  [笔记2](https://www.hankcs.com/nlp/cs224n-dependency-parsing.html)  [笔记3](https://zhuanlan.zhihu.com/p/66268929)

62. bootstrapping 自助法

63. ELMo  [ELMo最好用词向量Deep Contextualized Word Representations](https://zhuanlan.zhihu.com/p/38254332)

64. [Understanding Ranking Loss, Contrastive Loss, Margin Loss, Triplet Loss, Hinge Loss and all those confusing names](https://gombru.github.io/2019/04/03/ranking_loss/)

65. **Multi-instance Learning (MIL)** 多实例学习

    [知乎参考1](https://zhuanlan.zhihu.com/p/40812750)

    [南大周志华教授 miVLAD and miFV,](http://www.lamda.nju.edu.cn/CH.Data.ashx?AspxAutoDetectCookieSupport=1#code)
    
66. Snorkel - 基于弱监督学习的数据标注工具

    snorkel [Sonrkel--从0开始构建机器学习项目](https://zhuanlan.zhihu.com/p/55138499)

    [reference1](https://zhuanlan.zhihu.com/p/78699409)

    可以视为弱监督源的示例包括：

    * 领域启发式搜索，例如：常见模式、经验法则等
    * 已有的正确标注的数据，虽然不完全适用于当前的任务，但有一定的作用。这在传统上被称为远程监督
    * 不可靠的非专家标注人，例如：众包标注

    标准函数中编码了领域相关的推理规则，可以使用入正则表达式、经验规则等常见的模式进行标注。这样生成的标注是包含噪声的，并且可能彼此冲突。

    常见类型的标注函数：

    * 硬编码的推导：通常使用正则表达式
    * 语义结构：例如，使用[spacy](https://link.zhihu.com/?target=http%3A//sc.hubwiz.com/codebag/zh-spacy-model/)得到的依存关系结构
    * 远程监督：例如使用外部的知识库
    * 有噪声人工标注：例如众包标注
    * 外部模型：其他可以给出有用标注信号的模型

    当编写好标注函数后，Snorkel将利用这些不同的标注函数之间的冲突训练一个**标注模型（Label Model）**来估算不同标注函数的标注准确度。通过观察标注函数之间的彼此一致性，标注模型能够学习到每个监督源的准确度。

    例如，如果一个标注函数的标注结果总是得到其他标注函数的认可，那么这个标注函数将有一个高准确率，而如果一个标注函数总是与其他标注函数的结果不一致，那么这个标注函数将得到一个较低的准确率。通过整合所有的标注函数的投票结果（以其估算准确度作为权重），我们就可以为每个数据样本**分配一个包含噪声的标注（0~1之间）**，而不是一个硬标注（要么0，要么1）。

    接下来，当标注一个新的数据点时，每一个标注函数都会对分类进行投票：正、负或弃权。基于这些投票以及标注函数的估算精度，标注模型能够程序化到为上百万的数据点给出概率性标注。最终的目标是训练出一个可以超越标注函数的泛化能力的分类器.通过这种方法得到海量的低质量监督，然后使用统计技术处理有噪标注，我们可以训练出高质量的模型。

    [参考—Sonrkel--从0开始构建机器学习项目（完善中）](https://zhuanlan.zhihu.com/p/55138499)

    sample_and_sgd函数:    (计算在某一分布下的期望时，用蒙特卡洛积分近似， 去掉极限可以看成是采样点的均值)

    ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_j%5E%7B%28t%29%7D%3D%5Ctheta_j%5E%7B%28t-1%29%7D-%5Calpha%5Csum_%7Bk%3D1%7D%5E%7BN%7D%5Cleft%5C%7B+%5Cleft%5B+%5Cphi_j%28%5Chat%5Clambda_%7Bij%7D%5E%7B%28k%29%7D%2C+%5Chat+y_i%5E%7B%28k%29%7D%29+%5Cright%5D_%7B%7B%5Chat%5CLambda_i%5E%7B%28k%29%7D%2C+%5Chat+y_i%5E%7B%28k%29%7D+%5Csim+P%28%5CLambda_i%2C+y_i%29%7D%7D+-%5Cleft%5B+%5Cphi_j%28%5Clambda_%7Bij%7D%2C+%5Chat+y_i%5E%7B%28k%29%7D%29+%5Cright%5D_%7B%5Chat+y_i%5E%7B%28k%29%7D+%5Csim+P%28y_i%7C%5CLambda_i%29%7D+%5Cright%5C%7D)

    为了获得指定分布的样本点，我们需要进行采样。对于高维的联合分布，我们通常使用Gibbs采样算法

67. Gibbs采样算法  [Gibbs采样的原理](http://nitro.biosci.arizona.edu/courses/EEB596/handouts/Gibbs.pdf)

68. PGM 概率图模型 

    [An Introduction to Factor Graphs](https://link.zhihu.com/?target=http%3A//www.isiweb.ee.ethz.ch/papers/arch/aloe-2004-spmagffg.pdf)

    [Snorkel、PGM and sampling、SGD相关论文](https://zhuanlan.zhihu.com/p/55138499)

69. [Factor Graphs and the Sum-Product Algorithm](https://www.cs.toronto.edu/~radford/csc2506/factor.pdf)

70. 结构学习 structure learning   

    贝叶斯网路的学习包括：参数学习、[结构学习](http://www.huaxiaozhuan.com/统计学习/chapters/16_CRF.html)

    李宏毅 4 episode [Structured Learning 1: Introduction](https://www.youtube.com/watch?v=5OYu0vxXEv8&t=16s)

71. 半监督 [ML Lecture 12: Semi-supervised](https://www.youtube.com/watch?v=fX_guE7JNnY&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=21)

72. 利用生成模型从噪声标签源合成标签：

73. 玻尔兹曼机 RBM

    [受限玻尔兹曼机（RBM）原理总结](https://www.cnblogs.com/pinard/p/6530523.html)
    
74. 命名实体 NER  [命名实体识别 NER 论文综述](https://zhuanlan.zhihu.com/p/135453456)

75. Highway Networks  [Highway Networks及HBilstm Network](https://zhuanlan.zhihu.com/p/38130339)

76. entity  span  detection：找出 文本中指向同一实体的所有文段，这是因为，人们对同一个实体往往有多种不同的说法，如代词、省略词、别名等等。 [reference—基于span prediction的共指消解模型](https://zhuanlan.zhihu.com/p/126544790)

    <img src="%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/image-20200816172814181.png" alt="image-20200816172814181" style="zoom:80%;" />
    
77. [如何理解LSTM后接CRF？](https://www.zhihu.com/question/62399257)

    [CRF和LSTM 模型在序列标注上的优劣？](https://www.zhihu.com/question/46688107)

78. Viterbi(维特比算法)[HMM+Viterbi(维特比算法)+最短路径分析](https://zhuanlan.zhihu.com/p/59889195)

79. 知识图谱上的**实体消歧**  [一些关于NER任务调研的小思考](http://kugwzk.info/index.php/archives/3204)

80. 实体词典：entity dictionary

81. **What are Chunks ?**
    Chunks are made up of words and the kinds of words are defined using the part-of-speech tags. One can even define a pattern or words that can’t be a part of chuck and such words are known as chinks.

    **What are IOB tags ?**
    It is a format for chunks. These tags are similar to part-of-speech tags but provide can denote the inside, utside, and the beginning of a chunk. Not just noun phrase but multiple different chunk phrase types are allowed here.

    [code](https://www.geeksforgeeks.org/nlp-iob-tags/)

    [7. Extracting Information from Text—nltk](https://www.nltk.org/book/ch07.html)

82. [Karush-Kuhn-Tucker (KKT)条件](https://zhuanlan.zhihu.com/p/38163970)

83. [An Introduction to Statistical Learning with Applications in R](http://faculty.marshall.usc.edu/gareth-james/ISL/)

84. [collective classification](https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_140)  *jointly* determine the correct label assignments of all the objects in the network.

85. [ontology alignment](https://en.wikipedia.org/wiki/Ontology_alignment) 本体对齐

86. [personalized medicine](https://en.wikipedia.org/wiki/Personalized_medicine) 个性化医学

87. [opinion diffusion](https://link.springer.com/referenceworkentry/10.1007%2F978-1-4939-7131-2_379) 意见传播

88. trust in social networksolollllllllllllllllllll+

89. [graph summarization](https://www.jianshu.com/p/24fcf19b78da)

90. [t-norm](https://en.wikipedia.org/wiki/T-norm):    t-norm is a binary algebraic operation on the interval [0, 1],  三角范数，用于模糊逻辑

91. [ MPE inference](https://blog.csdn.net/jbb0523/article/details/79437497) 贝叶斯网络与最大可能解释(MPE)问题 MostProbable Explanation, MPE

92. 共识优化  [consensus optimization ](https://www.cvxpy.org/examples/applications/consensus_opt.html)

93. **knowledge distillation知识蒸馏：** [知乎1](https://zhuanlan.zhihu.com/p/51563760),  [知乎2](https://zhuanlan.zhihu.com/p/53864403),  [paper reading list](https://github.com/lhyfst/knowledge-distillation-papers)

94. 后验正则化（posterior regularization）方法

95. K-dimensional  probability  simplex

    ![img](%E6%A6%82%E5%BF%B5%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/3987bf963ef4ab9a7920cbb1d57056c3)

96. max-over-time 池化层，NLP中的CNN:  [参考1cnblog](https://blog.csdn.net/malefactor/article/details/51078135)  [Pooling vs Pooling-over-time](https://stackoverflow.com/questions/48549670/pooling-vs-pooling-over-time)

97. [Bidirectional LSTM-CNN (BLSTM-CNN) Training System](http://www.gabormelli.com/RKB/Bidirectional_LSTM-CNN_(BLSTM-CNN)_Training_System)

98. projected gradient descent (PGD)投影梯度下降 

    [投影梯度下降法解正则化问题：以Lasso回归为例](https://medium.com/數學-人工智慧與蟒蛇/投影梯度下降法解正則化問題-以lasso回歸為例-6fc70e4efe65)

    [Professor Bingsheng He—基于梯度投影的凸优化收缩算法和下降算法](http://maths.nju.edu.cn/~hebma/New-VS/SF10C-PG.pdf)

99. 利普西茨条件 

    [Lipschitz condition - Berkeley Math](https://math.berkeley.edu/~mgu/MA128ASpring2017/MA128ALectureWeek9.pdf)

    [非凸优化基石：Lipschitz Condition - 知乎](https://zhuanlan.zhihu.com/p/27554191)

    [Existence and Uniqueness 1 Lipschitz Conditions](https://mathcs.holycross.edu/~spl/old_courses/304_fall_2008/handouts/existunique.pdf)

100. [CAM 和 Grad-CAM](https://bindog.github.io/blog/2018/02/10/model-explanation/)

     热力图？Class Activation Mapping

#### **相关论文**

1. Joint event extraction via recurrent neural networks [论文解读](https://zhuanlan.zhihu.com/p/63215208)
2. [【论文笔记】Graph Convolutional Networks with Argument-Aware Pooling for Event Detection](https://www.cnblogs.com/kisetsu/p/11906681.html)      [笔记2](https://www.codenong.com/cs105391780/)
3. Jointly Extracting Event Triggers and Arguments by Dependency-Bridge RNN and Tensor-Based Argument Interaction   [笔记1](https://www.jianshu.com/p/9030d25215e3)   [笔记2](https://blog.csdn.net/JYZ4MFC/article/details/83309135)



